cluster_name: {{cluster_name}}

# The maximum number of workers nodes to launch in addition to the head node.
max_workers: {{num_nodes - 1}}
upscaling_speed: {{num_nodes - 1}}
idle_timeout_minutes: 60

provider:
  type: local
  head_ip: 169.229.48.124 # blaze.millennium.berkeley.edu
  # You may need to supply a public ip for the head node if you need
  # to run `ray up` from outside of the Ray cluster's network
  # (e.g. the cluster is in an AWS VPC and you're starting ray from your laptop)
  # This is useful when debugging the local node provider with cloud VMs.
  # external_head_ip: 169.229.48.124
  worker_ips: [havoc.millennium.berkeley.edu]
  # Optional when running automatic cluster management on prem. If you use a coordinator server,
  # then you can launch multiple autoscaling clusters on the same set of machines, and the coordinator
  # will assign individual nodes to clusters as needed.
  #    coordinator_address: "<host>:<port>"

auth:
  ssh_user: mluo
  ssh_private_key: ~/.ssh/sky-key

head_node_type: ray.head.default

# Format: `REMOTE_PATH : LOCAL_PATH`
file_mounts: {
  "{{sky_remote_path}}": "{{sky_local_path}}",
{%- for remote_path, local_path in credentials.items() %}
  "{{remote_path}}": "{{local_path}}",
{%- endfor %}
}

rsync_exclude: [
{%- for exclude_file in credential_excludes %}
  "{{exclude_file}}",
{%- endfor %}
]

# List of shell commands to run to set up nodes.
setup_commands:
  - (type -a python | grep -q python3) || echo 'alias python=python3' >> ~/.bashrc;
    (type -a pip | grep -q pip3) || echo 'alias pip=pip3' >> ~/.bashrc;
    pip3 install -U ray[default]=={{ray_version}} && mkdir -p ~/sky_workdir && mkdir -p ~/.sky/sky_app;
    which conda > /dev/null 2>&1 && conda init > /dev/null && conda config --set auto_activate_base false;
  - pip3 uninstall sky -y &> /dev/null; pip3 install {{sky_remote_path}}/*.whl && python3 -c "from sky.skylet.ray_patches import patch; patch()" # patch the buggy ray file

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
  # Set the ulimit as suggested by ray docs for performance. https://docs.ray.io/en/latest/cluster/guide.html?highlight=ulimit#system-configuration
  # Solution from https://discuss.ray.io/t/setting-ulimits-on-ec2-instances/590
  # This line is intentionally separated from the next line to reload the session after the ulimit is set.
  - sudo bash -c 'rm -rf /etc/security/limits.d; echo "* soft nofile 65535" >> /etc/security/limits.conf; echo "* hard nofile 65535" >> /etc/security/limits.conf;';
    (grep -Pzo -q "Host \*\n  StrictHostKeyChecking no" ~/.ssh/config) || printf "Host *\n  StrictHostKeyChecking no\n" >> ~/.ssh/config;
  - (ps aux | grep "-m sky.skylet.skylet" | grep -q python3) || nohup python3 -m sky.skylet.skylet >> ~/.sky/skylet.log 2>&1 & # Start skylet daemon. (Should not place it in the head_setup_commands, otherwise it will run before sky is installed.)
  - ray stop; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}}

worker_start_ray_commands:
  - sudo bash -c 'rm -rf /etc/security/limits.d; echo "* soft nofile 65535" >> /etc/security/limits.conf; echo "* hard nofile 65535" >> /etc/security/limits.conf;';
    (grep -Pzo -q "Host \*\n  StrictHostKeyChecking no" ~/.ssh/config) || printf "Host *\n  StrictHostKeyChecking no\n" >> ~/.ssh/config
  - ray stop; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}}
