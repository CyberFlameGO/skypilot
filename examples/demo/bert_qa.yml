name: bert_qa

resources:
    accelerators: V100:1
    cloud: aws
    use_spot: true
    spot_recovery: FAILOVER

num_nodes: 1

file_mounts:
    /checkpoint:
        name: sky-bert-qa-checkpoints
        mode: MOUNT

setup: |
    # Fill in your wandb key: copy from https://wandb.ai/authorize
    git clone https://github.com/huggingface/transformers.git
    cd transformers && git checkout v4.18.0
    pip install -e .
    cd examples/pytorch/question-answering/
    pip install -r requirements.txt
    pip install wandb

run: |
    EC2_AVAIL_ZONE=`curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone`
    GCP_AVAIL_ZONE=`curl -s "http://metadata.google.internal/computeMetadata/v1/instance/zone" -H "Metadata-Flavor: Google"`
    CLOUD="AWS"
    AZ=$EC2_AVAIL_ZONE
    if [[ "$AZ" == *"not found"* ]]; then
        CLOUD=GCP
        AZ=`echo $GCP_AVAIL_ZONE | sed -e "s/projects\/.*\/zones\///"`
    fi
    cd transformers/examples/pytorch/question-answering/
    python run_qa.py \
    --model_name_or_path bert-base-uncased \
    --dataset_name squad \
    --do_train \
    --do_eval \
    --per_device_train_batch_size 12 \
    --learning_rate 3e-5 \
    --num_train_epochs 50 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir /checkpoint/bert_qa/ \
    --report_to wandb \
    --run_name "$CLOUD $AZ" \
    --save_total_limit 10
