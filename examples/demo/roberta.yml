name: roberta

resources:
    accelerators: V100:4
    # use_spot: true
    # spot_recovery: FAILOVER

num_nodes: 1

file_mounts:
    /checkpoint:
        name: # Fill in your bucket name
        mode: MOUNT

    /data: s3://roberta-demo-data

setup: |
    # Fill in your wandb key: copy from https://wandb.ai/authorize
    echo export WANDB_API_KEY=[YOUR-WANDB-KEY] >> ~/.bashrc

    pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
    pip install wandb tensorboardX
    pip "numpy>=1.22.0"
    pip install -U numpy

    git clone https://github.com/facebookresearch/fairseq.git
    cd fairseq
    git checkout eb2d7862c29990e5be35ee227a6952ae21d621a1

    pip install --editable ./

    FILE=/data/train.idx
    if [ ! -f $FILE ]; then
        echo "Preparing dataset..."

        mkdir -p gpt2_bpe
        wget -O gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json
        wget -O gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe

        wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
        unzip wikitext-103-raw-v1.zip
        for SPLIT in train valid test; do \
            python -m examples.roberta.multiprocessing_bpe_encoder \
                --encoder-json gpt2_bpe/encoder.json \
                --vocab-bpe gpt2_bpe/vocab.bpe \
                --inputs wikitext-103-raw/wiki.${SPLIT}.raw \
                --outputs wikitext-103-raw/wiki.${SPLIT}.bpe \
                --keep-empty \
                --workers 60; \
        done

        wget -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt
        fairseq-preprocess \
            --only-source \
            --srcdict gpt2_bpe/dict.txt \
            --trainpref wikitext-103-raw/wiki.train.bpe \
            --validpref wikitext-103-raw/wiki.valid.bpe \
            --testpref wikitext-103-raw/wiki.test.bpe \
            --destdir /data \
            --workers 60
    fi

run: |
  cd fairseq
  DATA_DIR=/data
  OMP_NUM_THREADS=48 fairseq-hydra-train -m --config-dir examples/roberta/config/pretraining \
  --config-name base task.data=$DATA_DIR \
  common.wandb_project=fairseq \
  checkpoint.save_dir=/checkpoint \
  checkpoint.save_interval_updates=500 \
  checkpoint.keep_interval_updates=3 \
  common.log_format=tqdm